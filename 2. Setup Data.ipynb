{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99bd4ff3-09f0-459f-b749-75ed587de6eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 1. Setup Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a224c26c-fca0-4863-a343-c73144b497cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Attach the Notebook to a Cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ebdd1c05-3976-4643-a32e-dbcc1e0ccdb3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Compute\n",
    "You need `compute` in order to execute notebooks in Databricks. \n",
    "\n",
    "What is compute? When you run a notebook in Databricks, your code needs processing power and memory to perform calculations and work with data. Compute provides the underlying resources—like virtual machines or clusters—that actually execute your notebook commands, making it possible to analyze data and get results. \n",
    "\n",
    "For this workshop you may already have a `shared compute` configured or need to create your own `personal compute`\n",
    "\n",
    "> **If needed**, an example configuration is in this screenshot\n",
    "\n",
    "![compute](./screenshots/compute.png)\n",
    "\n",
    "> Optional\n",
    "* Its a good practice to **tag** your compute for cost tracking. Two default tags that are recommended are `COST_CENTER` and `PROJECT`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04796010-f8d1-4e4c-b5a9-8371f5596fcf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "In order to run code within a Databricks notebook, the notebook needs to be attached to a compute cluster. You can do this by clicking in the right hand corner on the button that says \"Connect\"\n",
    "\n",
    "\n",
    "![compute-connect](./screenshots/compute-connect.png)\n",
    "\n",
    "\n",
    "When you click connect, you will see a drop down of all compute clusters that are available for you to use. You can select any active resources that are available. If you do not see any options available, please reach out to your Databricks admin team\n",
    "\n",
    "\n",
    "![compute-starting](./screenshots/compute-starting.png)\n",
    "\n",
    "\n",
    "Once you select an option and see a cluster with a green dot assocaited with your notebook, then you are good to move on to the next steps\n",
    "\n",
    "\n",
    "![compute-green.png](./screenshots/compute-green.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7df2586d-d669-4029-99b1-d08459db86dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Create the Database/Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73d80f3f-7308-4b81-8ea1-604fd559cb5c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \"./1. Configure\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa7a2ac4-1ff5-4ed0-986e-a2b90155cd17",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sql_command = f\"CREATE SCHEMA IF NOT EXISTS {my_database}\"\n",
    "spark.sql(sql_command)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25c2ef69-5960-466b-af3a-388f0300592b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Load the Sample Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ceab5bec-8e32-4cac-834f-84e2ab13dc71",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Set the file location\n",
    "The block of code below is a way to work with the file system utilities in Databricks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "edbb0774-ec0d-4944-9dee-b8485b9d0146",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# This code gets the current path of this notebook\n",
    "context = dbutils.notebook.entry_point.getDbutils().notebook().getContext()\n",
    "notebook_path = context.notebookPath().get()\n",
    "\n",
    "# Since the notebook_path includes the notebook name, only keep up to the last '/' and ignore the filename\n",
    "folder_path = notebook_path[:notebook_path.rfind(\"/\")]\n",
    "\n",
    "csv_location = f\"file:/Workspace{folder_path}/data/taxi_raw.csv\"\n",
    "parquet_location = f\"file:/Workspace{folder_path}/data/taxi_raw.parquet\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e41fb51-88fe-495a-ad82-305555fdabb8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Read in a CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57be1ad0-d740-42ce-8c84-28695171633f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# read in the csv\n",
    "df = (\n",
    "    spark.read.format(\"csv\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .load(csv_location)\n",
    ")\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c82f6c02-547b-442d-97df-22a4075cf9a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Read in an existing Parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0866f79d-9def-41e6-91dc-d2d55a2bb900",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# read in the parquet\n",
    "df = (\n",
    "    spark.read.parquet(parquet_location)\n",
    ")\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e175c7c-a29f-4002-a31c-68c489d4ea1f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# save our dataframe as a table\n",
    "df.write.mode(\"overwrite\").saveAsTable(f\"{my_database}.bronze_trips\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "2. Setup Data",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}